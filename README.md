# ğŸ’¡ Simple--Lightweight-Chatbot

A super lightweight, local chatbot designed for desktop/laptop use â€” ideal for minimal setups, offline usage, and quick experimentation.

ğŸ› ï¸ This is my first chatbot project, later refined and optimized before being shared publicly for others to explore and learn from.

---

## ğŸš€ Features

- ğŸ’¬ Local LLM chatbot powered by Ollama
- ğŸ§  Custom persistent memory using memory.json
- ğŸ“ Chat and journal history stored in chat_history.json
- âš¡ Lightweight and clean architecture (accessible via local Wi-Fi)
- ğŸ“¦ Uses models pulled directly from Ollama (no LM Studio required)
- ğŸ–¥ï¸ Optimized for laptops and desktops
- ğŸ”Œ Offline-capable
- ğŸ”§ Built for simplicity and future extensibility

---

## ğŸï¸ Demo Preview

![Chatbot Demo](demo.gif)

---

## ğŸ–¥ï¸ System Requirements

Developed and tested on:

- Intel Core i5-13420H
- RTX 4050 Laptop GPU
- 16GB RAM
- Windows 11

Minimum recommended:

- Quad-core CPU
- 8â€“16GB RAM
- Optional discrete GPU
- Smaller quantized models (e.g. Phi-2) work well on modest hardware

---

## ğŸ“ Folder Structure

app.py                  â†’ Main Flask application  
brain.py                â†’ Core chatbot logic  
functions/              â†’ Modular helper functions  
â€¢ history_func.py       â†’ Conversation history  
â€¢ journal_func.py       â†’ Journaling logic  
â€¢ memory_func.py        â†’ Memory system  
â€¢ model_runner.py       â†’ Ollama interaction  
â€¢ prompt.py             â†’ Prompt construction  

templates/  
â€¢ index.html            â†’ Main chat UI  
â€¢ history.html          â†’ History & journal viewer  

static/  
â€¢ style.css             â†’ Styling  
â€¢ script.js             â†’ Frontend logic  

memory.json             â†’ Persistent chatbot memory  
chat_history.json       â†’ Stored chats & journals  
demo.gif                â†’ UI preview  

---

## ğŸ§  Ollama Model Setup

1. Install Ollama  
   https://ollama.com

2. Pull a model (example: Phi-2)

   ollama pull phi:2

3. Run the model

   ollama run phi:2

Phi-2 is used for its balance of speed and performance.  
You may also use models like mistral or llama3.

---

## ğŸ”§ How to Run the App

1. Clone the repository

   git clone https://github.com/Pranziss/Simple--Lightweight-Chatbot.git  
   cd Simple--Lightweight-Chatbot

2. (Optional) Create a virtual environment

   python -m venv venv  
   venv\Scripts\activate   (Windows)  
   source venv/bin/activate (Mac/Linux)

3. Install dependencies

   pip install flask

4. Start Ollama and run the app

   ollama run phi:2  
   python app.py

5. Open in browser

   http://localhost:5000

---

## âœ‰ï¸ Contact

Built with â¤ï¸ by Pranziss / yubedaoneineed

This is my first public chatbot project â€” feel free to fork, star â­, or reach out with feedback and ideas.
