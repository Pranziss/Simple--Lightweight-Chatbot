# üí° Very-Very-Light-Chatbot

A super lightweight, **local chatbot** project designed for desktop/laptop use ‚Äî ideal for minimal setups and quick testing.

> üõ†Ô∏è **This is my first chatbot project**, but it has been **refined and optimized** to be shared here publicly for others to explore and learn from.

---

## üöÄ Features

- üí¨ Local LLM chatbot powered by [Ollama](https://ollama.com)
- üß† Custom memory via `memory.json`
- üìù Chat and journal history saved in `chat_history.json`
- ‚ö° Super lightweight and clean architecture
- üì¶ Uses models directly pulled from Ollama (no LM Studio required)
- üñ•Ô∏è Optimized for laptops and desktops (offline-capable)
- üîß Built for simplicity and extensibility

---

## üéûÔ∏è Demo Preview

Here‚Äôs a look at the chatbot running locally:

![Chatbot Demo](demo.gif)

---

## üñ•Ô∏è System Requirements

> üí° This chatbot was developed and tested on the following specs:

- üîπ Intel Core **i5-13420H**
- üîπ **RTX 4050** Laptop GPU
- üîπ **16GB RAM**
- üîπ Windows 11

Minimum recommended:

- ‚úÖ Quad-core CPU
- ‚úÖ 8‚Äì16GB RAM
- ‚úÖ Optional: Discrete GPU (for faster performance)
- ‚úÖ Smaller quantized models (like Phi-2) work well on modest hardware

---

## üìÅ Folder Structure

```
‚îú‚îÄ‚îÄ app.py                  # Main Flask app
‚îú‚îÄ‚îÄ brain.py                # Core chatbot flow and logic
‚îú‚îÄ‚îÄ functions/              # Modular helper functions
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ history_func.py     # Manages conversation history
‚îÇ   ‚îú‚îÄ‚îÄ journal_func.py     # Handles journaling features
‚îÇ   ‚îú‚îÄ‚îÄ memory_func.py      # Memory system logic
‚îÇ   ‚îú‚îÄ‚îÄ model_runner.py     # Interacts with LLM via Ollama
‚îÇ   ‚îî‚îÄ‚îÄ prompt.py           # Prompt creation/injection
‚îú‚îÄ‚îÄ templates/
‚îÇ   ‚îú‚îÄ‚îÄ index.html          # Main chat UI
‚îÇ   ‚îî‚îÄ‚îÄ history.html        # Journal/history viewer
‚îú‚îÄ‚îÄ static/
‚îÇ   ‚îú‚îÄ‚îÄ style.css           # Basic CSS styling
‚îÇ   ‚îî‚îÄ‚îÄ script.js           # Frontend behavior
‚îú‚îÄ‚îÄ memory.json             # Persistent chatbot memory
‚îú‚îÄ‚îÄ chat_history.json       # Stored conversation/journal logs
‚îú‚îÄ‚îÄ demo.gif                # Optional: Local UI preview
```

---

## üß† Ollama-Based Model Setup

This project skips the LM Studio step and directly uses Ollama to pull and run the model:

### üîπ Step 1: Install Ollama

Download and install from [https://ollama.com](https://ollama.com)

### üîπ Step 2: Pull Model (e.g. Phi-2)

```bash
ollama pull phi:2
```

> üß† This project uses the **Phi-2** model for its balance of speed and performance. Feel free to swap in other models (like `mistral`, `llama3`, or others).

### üîπ Step 3: Run the Model

```bash
ollama run phi:2
```

Your local model is now running and ready for chat.

---

## üîß How to Run the App

### 1. Clone the Project

```bash
git clone https://github.com/yourusername/very-very-light-chatbot.git
cd very-very-light-chatbot
```

### 2. Create Virtual Environment (Optional)

```bash
python -m venv venv
venv\Scripts\activate   # On Windows
# OR
source venv/bin/activate  # On Mac/Linux
```

### 3. Install Flask

```bash
pip install flask
```

### 4. Start Ollama + Run Flask App

Make sure Ollama is running your model (`phi:2` or another):

```bash
ollama run phi:2
python app.py
```

### 5. Open in Browser

Go to:

```
http://localhost:11434
```

---

## ‚úÖ Check Your Versions

```bash
python --version
flask --version
ollama --version
```

## ‚úâÔ∏è Contact

Built with ‚ù§Ô∏è by **yubedaoneineed/Pranziss**

This is my first public chatbot project ‚Äî feel free to fork, star ‚≠ê, or reach out with feedback or ideas!
